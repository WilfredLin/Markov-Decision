import numpy as np

class MarkovDecisionProcess:
    def __init__(self, states, actions, transition_probabilities, rewards, discount_factor=0.9):
        self.states = states
        self.actions = actions
        self.transition_probabilities = transition_probabilities  # P(s'|s,a)
        self.rewards = rewards  # R(s,a)
        self.discount_factor = discount_factor

    def value_iteration(self, theta=1e-6):
        V = np.zeros(len(self.states))  # Initialize value function
        while True:
            delta = 0
            for s in range(len(self.states)):
                v = V[s]
                V[s] = max(self.compute_action_value(s, a) for a in range(len(self.actions)))
                delta = max(delta, abs(v - V[s]))
            if delta < theta:
                break
        return V

    def compute_action_value(self, state, action):
        return sum(self.transition_probabilities[state][action][s_prime] * 
                   (self.rewards[state][action] + self.discount_factor * V[s_prime]) 
                   for s_prime in range(len(self.states)))

    def policy_extraction(self, V):
        policy = np.zeros(len(self.states), dtype=int)
        for s in range(len(self.states)):
            policy[s] = np.argmax([self.compute_action_value(s, a) for a in range(len(self.actions))])
        return policy

# Example usage:
states = [0, 1, 2]  # Example states
actions = [0, 1]  # Example actions
transition_probabilities = [  # P(s'|s,a)
    [[0.8, 0.2, 0.0], [0.0, 0.9, 0.1]],  # From state 0
    [[0.0, 0.3, 0.7], [0.6, 0.4, 0.0]],  # From state 1
    [[0.4, 0.6, 0.0], [0.0, 0.0, 1.0]],  # From state 2
]
rewards = [  # R(s,a)
    [5, 10],  # Rewards for state 0
    [2, 1],   # Rewards for state 1
    [0, 0],   # Rewards for state 2
]
mdp = MarkovDecisionProcess(states, actions, transition_probabilities, rewards)
V = mdp.value_iteration()
policy = mdp.policy_extraction(V)
print("Value Function:", V)
print("Optimal Policy:", policy)
